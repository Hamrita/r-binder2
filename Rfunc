library(keras)
library(tensorflow)

##########################
lags <- function(x, k){
    
    lagged =  c(rep(NA, k), x[1:(length(x)-k)])
    DF = as.data.frame(cbind(lagged, x))
    colnames(DF) <- c( paste0('x-', k), 'x')
    DF[is.na(DF)] <- 0
    return(DF)
  }

  #############################
   ## scale data
  normalize <- function(train, test, feature_range = c(0, 1)) {
    x = train
    fr_min = feature_range[1]
    fr_max = feature_range[2]
    std_train = ((x - min(x) ) / (max(x) - min(x)  ))
    std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
    
    scaled_train = std_train *(fr_max -fr_min) + fr_min
    scaled_test = std_test *(fr_max -fr_min) + fr_min
    
    return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
    
  }
  
  ##################################
  inverter = function(scaled, scaler, feature_range = c(0, 1)){
    min = scaler[1]
    max = scaler[2]
    n = length(scaled)
    mins = feature_range[1]
    maxs = feature_range[2]
    inverted_dfs = numeric(n)
    
    for( i in 1:n){
      X = (scaled[i]- mins)/(maxs - mins)
      rawValues = X *(max - min) + min
      inverted_dfs[i] <- rawValues
    }
    return(inverted_dfs)
  }

  #############################################

  lstmFit=function(series, k=1, Eposh=50, p=0.66){
  	# transform data to stationarity
  	diffed = diff(series, differences = 1)
  	# creat a lagged dataset (supervise learning)
  	supervised = lags(diffed, k)
  	# split data ( train and test data)
  	N = nrow(supervised)
    n = round(N *p, digits = 0)
    train = supervised[1:n, ]
    test  = supervised[(n+1):N,  ]
    # scale data
    Scaled = normalize(train, test, c(-1, 1))
  
    y_train = Scaled$scaled_train[, 2]
    x_train = Scaled$scaled_train[, 1]
  
    y_test = Scaled$scaled_test[, 2]
    x_test = Scaled$scaled_test[, 1]

     ## fit the model
  
    dim(x_train) <- c(length(x_train), 1, 1)
    dim(x_train)
    X_shape2 = dim(x_train)[2]
    X_shape3 = dim(x_train)[3]
    batch_size = 1
    units = 1
  
    model <- keras_model_sequential() 
    model%>%
    layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%>%
    layer_dense(units = 1)

    model %>% compile(
    loss = 'mean_squared_error',
    optimizer = optimizer_adam( lr= 0.02 , decay = 1e-6 ),  
    metrics = c('accuracy')
  )
  nb_epoch = Epochs   
  for(i in 1:nb_epoch ){
    model %>% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
    model %>% reset_states()
  }
  L = length(x_test)
  dim(x_test) = c(L, 1, 1)
  
  scaler = Scaled$scaler
  fitted_tr=numeric(dim(x_train)[1])
  predictions = numeric(L)
  
  for(i in 1:L){
    X = x_test[i , , ]
    dim(X) = c(1,1,1)
    # forecast
    yhat = model %>% predict(X, batch_size=batch_size)
    
    # invert scaling
    yhat = inverter(yhat, scaler,  c(-1, 1))
    
    # invert differencing
    yhat  = yhat + Series[(n+i)] 
    
    # save prediction
    predictions[i] <- yhat
  }
  kk=dim(x_train)[1]
  for(i in 1:kk){
    X = x_train[i , , ]
    dim(X) = c(1,1,1)
    # fitt
    yhat = model %>% predict(X, batch_size=batch_size)
    
    # invert scaling
    yhat = inverter(yhat, scaler,  c(-1, 1))
    
    # invert differencing
    yhat  = yhat + Series[(n+i)] 
    
    # save prediction
    fitted_tr[i] <- yhat
  }
 res=list()
 res$fitted_tr=fitted_tr
 res$pred=predictions
 res$x_train=x_train[,1,1]


##################################
MRA=function(x,wf="la8",J=4){
	nn=length(x)
	dec=waveslim::mra(x,wf=wf,J=J)
	series=matrix(unlist(dec), nr=nn)
	return(series)
}

mraARIMA=function(x,wf="la8", J=4, p=3, q=3, h=10){
 all_pred=0; all_forecast=0
 series=MRA(x,wf=wf,J=J)
 for(ii in 1:ncol(series)){
 	ts=0
 	ts=series[,ii]
 	mra_fit=forecast::auto.arima(x=as.ts(ts), d=NA,D=NA,
 		   max.p=p, max.q=q, stationary=F,seasonal=F, ic=c("aic"),
 		   allowdrift=F, stepwise=T)
 	mraPredict=mra_fit$fitted
 	mraForecast=forecast::forecast(mra_fit,h=h)
 	all_pred=cbind(all_pred,mraPredict)
 	all_forecast=cbind(all_forecast, as.matrix(mraForecast$mean))
 }
 Forecast=rowSums(all_forecast, na.rm=T)
 Pred=rowSums(all_pred, na.rm=T)
 return(list(Forecast=Forecast, Fitted=Pred))
}

########
# MRA ann

mraANN=function(x,wf="la8", J=4, p=3, P=1, size=2, h=10){
	# p: nonseasonal lag
	# P: seasonal lag
	# size: hidden Size of the hidden layer

 all_pred=0; all_forecast=0
 series=MRA(x,wf=wf,J=J)
 for(ii in 1:ncol(series)){
 	ts=0
 	ts=series[,ii]
 	mra_fit=forecast::nnetar(y=as.ts(ts), p=p, P=P, size=size)
 	mraPredict=mra_fit$fitted
 	mraForecast=forecast::forecast(mra_fit,h=h)
 	all_pred=cbind(all_pred,mraPredict)
 	all_forecast=cbind(all_forecast, as.matrix(mraForecast$mean))
 }
 Forecast=rowSums(all_forecast, na.rm=T)
 Pred=rowSums(all_pred, na.rm=T)
 return(list(Forecast=Forecast, Fitted=Pred))
}

## Recherche des meilleurs val de p et size au sens RMSE

bestMod=function(series,pMax=10,sizeMax=10){
	wwf=c("haar","d4","d6","d8","la8","d16","la16")
	tt=expand.grid(1:pMax,1:sizeMax,wwf)
	n=NROW(tt)
	rmse_ij=0
	for(i in 1:n){
		fit=mraANN(series,wf=tt[i,3],p=tt[i,1],size=tt[i,2])$Fitted
		rmse_ij=c(rmse_ij, Metrics::rmse(series,fit))
	}
	rmseij=rmse_ij[-1]
	idx=which.min(rmseij)
	bestPar=tt[idx,1:3]
	cat("\n")
	cat("======================================================================\n")
	cat(" Best model with RMSE :", "p = ", bestPar$Var1, "size = ", bestPar$Var2, "\n")
	cat(" Best wavelet filter: ", bestPar$Var3[idx],"\n")
    cat("======================================================================\n")
	return(invisible(list(RMSE=cbind(tt, RMSE=rmse_ij[-1]), bestPar=idx)))
}
